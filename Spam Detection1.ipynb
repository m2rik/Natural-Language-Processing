{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the spam detection problem, there are 2 classes: C1 which is the no-spam (ham) class and C2 which is the spam class. X is essentially each email present in the training data. To convert X into a machine-readable form (number), we basically need to convert X into a vector. We achieve it by the following way:\n",
    "\n",
    "# Create an ordered list of all the words in the vocabulary. For instance, suppose we have the following words in the vocabulary: [lottery, how, won, offer, thanks, the, you].\n",
    "# To convert an email into a vector, map out the number of times each word occurs in that email. For instance, consider the following email: you won the lottery. The vector form of the above email would be [1, 0, 1, 0, 0, 1, 1].\n",
    "# Now that we have mapped each email into a vector, we can apply the Naive Bayes algorithm on the data. Observe that in the above process, we assumed that each word is produced independent of each other and we discarded the ordering of words in the email. This exactly is the “Naive” assumption and that’s how we plan to apply the Naive Bayes algorithm to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "# data=data.drop(['2','3','4'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":\"class\", \"v2\":\"text\"})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['length'] = data['text'].apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2c2aae74a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data['class'],data['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of each text messages to see whether it is correlated with the text classified as a spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in text.split()\n",
    "        if word.lower is not in \n",
    "     return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#with countvectorizer\n",
    "\n",
    "# You must always have a 1D object so CountVectorizer can turn into a 2D object for the model to be built on\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer()#lower_case=True made all lowercase\n",
    "# vect.fit(train) learns the vocabulary of the training data\n",
    "# vect.transform(train) uses the fitted vocabulary to build a document-term matrix from the training data\n",
    "X1 = vectorizer.fit_transform(data['text'].copy())#transform training data into a 'document-term matrix'\n",
    "\n",
    "# document = rows\n",
    "# term = columns\n",
    "\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X1.toarray())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, data['class'], test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9826555023923444"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bn =BernoulliNB(alpha=0.2)\n",
    "bn.fit(features_train, labels_train)\n",
    "prediction1 = bn.predict(features_test)\n",
    "accuracy_score(labels_test,prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1426   14]\n",
      " [  15  217]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(labels_test, prediction1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build one also with CountVectorizer or even a Binomial Vectorizer to try a binomial NB\n",
    "# It has been shown that binomialNB also works well for spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFeatures = data['text'].copy()\n",
    "textFeatures = textFeatures.apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go jurong point crazi avail bugi n great world...\n",
       "1                                  ok lar joke wif u oni \n",
       "2       free entri 2 wkli comp win fa cup final tkts 2...\n",
       "3                    u dun say earli hor u c alreadi say \n",
       "4              nah dont think goe usf live around though \n",
       "5       freemsg hey darl 3 week word back id like fun ...\n",
       "6          even brother like speak treat like aid patent \n",
       "7       per request mell mell oru minnaminungint nurun...\n",
       "8       winner valu network custom select receivea å£9...\n",
       "9       mobil 11 month u r entitl updat latest colour ...\n",
       "10      im gonna home soon dont want talk stuff anymor...\n",
       "11      six chanc win cash 100 20000 pound txt csh11 s...\n",
       "12      urgent 1 week free membership å£100000 prize j...\n",
       "13      ive search right word thank breather promis wo...\n",
       "14                                           date sunday \n",
       "15      xxxmobilemovieclub use credit click wap link n...\n",
       "16                                          oh kim watch \n",
       "17      eh u rememb 2 spell name yes v naughti make v ...\n",
       "18              fine thatåõ way u feel thatåõ way gota b \n",
       "19      england v macedonia dont miss goalsteam news t...\n",
       "20                                    serious spell name \n",
       "21                       iû÷m go tri 2 month ha ha joke \n",
       "22                        ì pay first lar da stock comin \n",
       "23      aft finish lunch go str lor ard 3 smth lor u f...\n",
       "24                    ffffffffff alright way meet sooner \n",
       "25      forc eat slice im realli hungri tho suck mark ...\n",
       "26                                     lol alway convinc \n",
       "27      catch bus fri egg make tea eat mom left dinner...\n",
       "28          im back amp pack car ill let know there room \n",
       "29                   ahhh work vagu rememb feel like lol \n",
       "                              ...                        \n",
       "5542                          armand say get ass epsilon \n",
       "5543                 u still havent got urself jacket ah \n",
       "5544    im take derek amp taylor walmart im back time ...\n",
       "5545                              hi durban still number \n",
       "5546                              ic lotta childporn car \n",
       "5547    contract mobil 11 mnths latest motorola nokia ...\n",
       "5548                                       tri weekend v \n",
       "5549    know wot peopl wear shirt jumper hat belt know...\n",
       "5550                                 cool time think get \n",
       "5551                     wen get spiritu deep that great \n",
       "5552    safe trip nigeria wish happi soon compani shar...\n",
       "5553                                 hahahaus brain dear \n",
       "5554    well keep mind ive got enough gas one round tr...\n",
       "5555    yeh indian nice tho kane bit shud go 4 drink s...\n",
       "5556                      yes that u text pshewmiss much \n",
       "5557    meant calcul ltgt unit ltgt school realli expe...\n",
       "5558                                sorri ill call later \n",
       "5559                 arent next ltgt hour imma flip shit \n",
       "5560                                anyth lor juz us lor \n",
       "5561               get dump heap mom decid come low bore \n",
       "5562    ok lor soni ericsson salesman ask shuhui say q...\n",
       "5563                                  ard 6 like dat lor \n",
       "5564               dont wait til least wednesday see get \n",
       "5565                                             huh lei \n",
       "5566    remind o2 get 250 pound free call credit detai...\n",
       "5567    2nd time tri 2 contact u u å£750 pound prize 2...\n",
       "5568                             ì b go esplanad fr home \n",
       "5569                             piti mood soani suggest \n",
       "5570    guy bitch act like id interest buy someth els ...\n",
       "5571                                      rofl true name \n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\"english\")\n",
    "features = vectorizer.fit_transform(textFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "p=features.toarray()\n",
    "print(len(p))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "p1=np.array(p[1:2])\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=[]\n",
    "for i in p1:\n",
    "    for j in i:\n",
    "        o.append(j.round(10))\n",
    "o[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, until, jurong, point, ,, crazy.., Availab...\n",
       "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
       "2       [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
       "3       [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
       "4       [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
       "5       [FreeMsg, Hey, there, darling, it, 's, been, 3...\n",
       "6       [Even, my, brother, is, not, like, to, speak, ...\n",
       "7       [As, per, your, request, 'Melle, Melle, (, Oru...\n",
       "8       [WINNER, !, !, As, a, valued, network, custome...\n",
       "9       [Had, your, mobile, 11, months, or, more, ?, U...\n",
       "10      [I, 'm, gon, na, be, home, soon, and, i, do, n...\n",
       "11      [SIX, chances, to, win, CASH, !, From, 100, to...\n",
       "12      [URGENT, !, You, have, won, a, 1, week, FREE, ...\n",
       "13      [I, 've, been, searching, for, the, right, wor...\n",
       "14       [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL, !, !]\n",
       "15      [XXXMobileMovieClub, :, To, use, your, credit,...\n",
       "16              [Oh, k, ..., i, 'm, watching, here, :, )]\n",
       "17      [Eh, u, remember, how, 2, spell, his, name, .....\n",
       "18      [Fine, if, thatåÕs, the, way, u, feel, ., That...\n",
       "19      [England, v, Macedonia, -, dont, miss, the, go...\n",
       "20      [Is, that, seriously, how, you, spell, his, na...\n",
       "21      [IÛ÷m, going, to, try, for, 2, months, ha, ha...\n",
       "22      [So, Ì_, pay, first, lar, ..., Then, when, is,...\n",
       "23      [Aft, i, finish, my, lunch, then, i, go, str, ...\n",
       "24      [Ffffffffff, ., Alright, no, way, I, can, meet...\n",
       "25      [Just, forced, myself, to, eat, a, slice, ., I...\n",
       "26                 [Lol, your, always, so, convincing, .]\n",
       "27      [Did, you, catch, the, bus, ?, Are, you, fryin...\n",
       "28      [I, 'm, back, &, amp, ;, we, 're, packing, the...\n",
       "29      [Ahhh, ., Work, ., I, vaguely, remember, that,...\n",
       "                              ...                        \n",
       "5542    [Armand, says, get, your, ass, over, to, epsilon]\n",
       "5543    [U, still, havent, got, urself, a, jacket, ah, ?]\n",
       "5544    [I, 'm, taking, derek, &, amp, ;, taylor, to, ...\n",
       "5545    [Hi, its, in, durban, are, you, still, on, thi...\n",
       "5546    [Ic, ., There, are, a, lotta, childporn, cars,...\n",
       "5547    [Had, your, contract, mobile, 11, Mnths, ?, La...\n",
       "5548      [No, ,, I, was, trying, it, all, weekend, ;, V]\n",
       "5549    [You, know, ,, wot, people, wear, ., T, shirts...\n",
       "5550    [Cool, ,, what, time, you, think, you, can, ge...\n",
       "5551    [Wen, did, you, get, so, spiritual, and, deep,...\n",
       "5552    [Have, a, safe, trip, to, Nigeria, ., Wish, yo...\n",
       "5553                     [Hahaha..use, your, brain, dear]\n",
       "5554    [Well, keep, in, mind, I, 've, only, got, enou...\n",
       "5555    [Yeh, ., Indians, was, nice, ., Tho, it, did, ...\n",
       "5556    [Yes, i, have, ., So, that, 's, why, u, texted...\n",
       "5557    [No, ., I, meant, the, calculation, is, the, s...\n",
       "5558                      [Sorry, ,, I, 'll, call, later]\n",
       "5559    [if, you, are, n't, here, in, the, next, &, lt...\n",
       "5560        [Anything, lor, ., Juz, both, of, us, lor, .]\n",
       "5561    [Get, me, out, of, this, dump, heap, ., My, mo...\n",
       "5562    [Ok, lor, ..., Sony, ericsson, salesman, ..., ...\n",
       "5563                          [Ard, 6, like, dat, lor, .]\n",
       "5564    [Why, do, n't, you, wait, 'til, at, least, wed...\n",
       "5565                                   [Huh, y, lei, ...]\n",
       "5566    [REMINDER, FROM, O2, :, To, get, 2.50, pounds,...\n",
       "5567    [This, is, the, 2nd, time, we, have, tried, 2,...\n",
       "5568     [Will, Ì_, b, going, to, esplanade, fr, home, ?]\n",
       "5569    [Pity, ,, *, was, in, mood, for, that, ., So, ...\n",
       "5570    [The, guy, did, some, bitching, but, I, acted,...\n",
       "5571                  [Rofl, ., Its, true, to, its, name]\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens=data['text'].apply(lambda x: word_tokenize(x))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrammed=tokens.apply(lambda x : list(ngrams(x,2)))\n",
    "# Danger of using two-grams (bigrams)\n",
    "# Number of features will grow really quickly\n",
    "# You need to know if you are throwing in noise or signal into your model\n",
    "# You need to check if there's potential value for improving the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(Go, until), (until, jurong), (jurong, point)...\n",
       "1       [(Ok, lar), (lar, ...), (..., Joking), (Joking...\n",
       "2       [(Free, entry), (entry, in), (in, 2), (2, a), ...\n",
       "3       [(U, dun), (dun, say), (say, so), (so, early),...\n",
       "4       [(Nah, I), (I, do), (do, n't), (n't, think), (...\n",
       "5       [(FreeMsg, Hey), (Hey, there), (there, darling...\n",
       "6       [(Even, my), (my, brother), (brother, is), (is...\n",
       "7       [(As, per), (per, your), (your, request), (req...\n",
       "8       [(WINNER, !), (!, !), (!, As), (As, a), (a, va...\n",
       "9       [(Had, your), (your, mobile), (mobile, 11), (1...\n",
       "10      [(I, 'm), ('m, gon), (gon, na), (na, be), (be,...\n",
       "11      [(SIX, chances), (chances, to), (to, win), (wi...\n",
       "12      [(URGENT, !), (!, You), (You, have), (have, wo...\n",
       "13      [(I, 've), ('ve, been), (been, searching), (se...\n",
       "14      [(I, HAVE), (HAVE, A), (A, DATE), (DATE, ON), ...\n",
       "15      [(XXXMobileMovieClub, :), (:, To), (To, use), ...\n",
       "16      [(Oh, k), (k, ...), (..., i), (i, 'm), ('m, wa...\n",
       "17      [(Eh, u), (u, remember), (remember, how), (how...\n",
       "18      [(Fine, if), (if, thatåÕs), (thatåÕs, the), (t...\n",
       "19      [(England, v), (v, Macedonia), (Macedonia, -),...\n",
       "20      [(Is, that), (that, seriously), (seriously, ho...\n",
       "21      [(IÛ÷m, going), (going, to), (to, try), (try,...\n",
       "22      [(So, Ì_), (Ì_, pay), (pay, first), (first, la...\n",
       "23      [(Aft, i), (i, finish), (finish, my), (my, lun...\n",
       "24      [(Ffffffffff, .), (., Alright), (Alright, no),...\n",
       "25      [(Just, forced), (forced, myself), (myself, to...\n",
       "26      [(Lol, your), (your, always), (always, so), (s...\n",
       "27      [(Did, you), (you, catch), (catch, the), (the,...\n",
       "28      [(I, 'm), ('m, back), (back, &), (&, amp), (am...\n",
       "29      [(Ahhh, .), (., Work), (Work, .), (., I), (I, ...\n",
       "                              ...                        \n",
       "5542    [(Armand, says), (says, get), (get, your), (yo...\n",
       "5543    [(U, still), (still, havent), (havent, got), (...\n",
       "5544    [(I, 'm), ('m, taking), (taking, derek), (dere...\n",
       "5545    [(Hi, its), (its, in), (in, durban), (durban, ...\n",
       "5546    [(Ic, .), (., There), (There, are), (are, a), ...\n",
       "5547    [(Had, your), (your, contract), (contract, mob...\n",
       "5548    [(No, ,), (,, I), (I, was), (was, trying), (tr...\n",
       "5549    [(You, know), (know, ,), (,, wot), (wot, peopl...\n",
       "5550    [(Cool, ,), (,, what), (what, time), (time, yo...\n",
       "5551    [(Wen, did), (did, you), (you, get), (get, so)...\n",
       "5552    [(Have, a), (a, safe), (safe, trip), (trip, to...\n",
       "5553    [(Hahaha..use, your), (your, brain), (brain, d...\n",
       "5554    [(Well, keep), (keep, in), (in, mind), (mind, ...\n",
       "5555    [(Yeh, .), (., Indians), (Indians, was), (was,...\n",
       "5556    [(Yes, i), (i, have), (have, .), (., So), (So,...\n",
       "5557    [(No, .), (., I), (I, meant), (meant, the), (t...\n",
       "5558    [(Sorry, ,), (,, I), (I, 'll), ('ll, call), (c...\n",
       "5559    [(if, you), (you, are), (are, n't), (n't, here...\n",
       "5560    [(Anything, lor), (lor, .), (., Juz), (Juz, bo...\n",
       "5561    [(Get, me), (me, out), (out, of), (of, this), ...\n",
       "5562    [(Ok, lor), (lor, ...), (..., Sony), (Sony, er...\n",
       "5563    [(Ard, 6), (6, like), (like, dat), (dat, lor),...\n",
       "5564    [(Why, do), (do, n't), (n't, you), (you, wait)...\n",
       "5565                     [(Huh, y), (y, lei), (lei, ...)]\n",
       "5566    [(REMINDER, FROM), (FROM, O2), (O2, :), (:, To...\n",
       "5567    [(This, is), (is, the), (the, 2nd), (2nd, time...\n",
       "5568    [(Will, Ì_), (Ì_, b), (b, going), (going, to),...\n",
       "5569    [(Pity, ,), (,, *), (*, was), (was, in), (in, ...\n",
       "5570    [(The, guy), (guy, did), (did, some), (some, b...\n",
       "5571    [(Rofl, .), (., Its), (Its, true), (true, to),...\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrammed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try visualizing the tf-idf vectorizer so people get a feel of what is being generated, consider bigrams too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The multinomial Naive Bayes classifier is suitable for classification with discrete features \n",
    "#(e.g., word counts for text classification). \n",
    "#The multinomial distribution normally requires integer feature counts. \n",
    "#However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850478468899522"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "mnb.fit(features_train, labels_train)\n",
    "prediction = mnb.predict(features_test)\n",
    "accuracy_score(labels_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3385.,  515.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1672x8037 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14032 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98535561, 0.01464439],\n",
       "       [0.9928813 , 0.0071187 ],\n",
       "       [0.98230745, 0.01769255],\n",
       "       ...,\n",
       "       [0.99551829, 0.00448171],\n",
       "       [0.98873388, 0.01126612],\n",
       "       [0.99734973, 0.00265027]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict_proba(features_test)#Return probability estimates for the test vector X.\n",
    "# left Column: probability class 0\n",
    "# right C: probability class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.82318376, -10.82318376, -10.82318376, ..., -10.82318376,\n",
       "         -8.43164714, -10.82318376],\n",
       "       [ -9.7768633 ,  -8.901744  ,  -8.7361935 , ...,  -9.7768633 ,\n",
       "         -9.7768633 ,  -9.7768633 ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     1440\n",
      "spam     232\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# examine class distribution\n",
    "print(labels_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1429   11]\n",
      " [  14  218]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(labels_test, prediction))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "             correct    not correct\n",
    "selected      tp        fp\n",
    "notselected   fn        tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.99      0.99      0.99      1440\n",
      "        ham       0.95      0.94      0.95       232\n",
      "\n",
      "avg / total       0.98      0.99      0.99      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['spam', 'ham']\n",
    "print(classification_report(labels_test, prediction, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction #on features test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='english',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8037"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary of X_train\n",
    "X_train_tokens = vectorizer.get_feature_names()\n",
    "len(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['008704050406', '0089mi', '0121', '01223585236', '01223585334', '0125698789', '02', '020603', '0207', '02070836089', '02072069400', '02073162414', '02085076972', '020903', '021', '050703', '0578', '06', '060505', '061104', '07008009200', '07046744435', '07090201529', '07090298926', '07099833605', '071104', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '07753741225', '0776xxxxxxx', '07786200117', '077xxx', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '07821230901', '0784987', '0789xxxxxxx', '0794674629107880867867', '0796xxxxxx', '07973788240', '07xxxxxxxxx', '0800', '08000407165']\n"
     ]
    }
   ],
   "source": [
    "# examine the first 50 tokens\n",
    "print(X_train_tokens[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the last 50 tokens\n",
    "print(X_train_tokens[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "# trailing underscore - learned during fitting\n",
    "mnb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8037)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows represent classes, columns represent tokens\n",
    "mnb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.        , 1.98605534,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all HAM messages\n",
    "ham_token_count = mnb.feature_count_[0, :]\n",
    "ham_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.2798323 , 0.36622253, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all SPAM messages\n",
    "spam_token_count = mnb.feature_count_[1, :]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008704050406</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0089mi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585334</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham      spam\n",
       "token                      \n",
       "008704050406  0.0  0.000000\n",
       "0089mi        0.0  0.279832\n",
       "0121          0.0  0.366223\n",
       "01223585236   0.0  0.000000\n",
       "01223585334   0.0  0.239626"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, 'spam':spam_token_count}).set_index('token')\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3385.,  515.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "mnb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>moan</th>\n",
       "      <td>2.756161</td>\n",
       "      <td>1.232211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thur</th>\n",
       "      <td>2.000235</td>\n",
       "      <td>1.270366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yago</th>\n",
       "      <td>1.723333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missi</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifeand</th>\n",
       "      <td>1.255583</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham      spam\n",
       "token                      \n",
       "moan     2.756161  1.232211\n",
       "thur     2.000235  1.270366\n",
       "yago     1.723333  1.000000\n",
       "missi    1.000000  1.000000\n",
       "lifeand  1.255583  1.000000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 to ham and spam counts to avoid dividing by 0\n",
    "tokens['ham'] = tokens.ham + 1\n",
    "tokens['spam'] = tokens.spam + 1\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>moan</th>\n",
       "      <td>2.756161</td>\n",
       "      <td>1.232211</td>\n",
       "      <td>0.447075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thur</th>\n",
       "      <td>2.000235</td>\n",
       "      <td>1.270366</td>\n",
       "      <td>0.635109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yago</th>\n",
       "      <td>1.723333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.580271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missi</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifeand</th>\n",
       "      <td>1.255583</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham      spam  spam_ratio\n",
       "token                                  \n",
       "moan     2.756161  1.232211    0.447075\n",
       "thur     2.000235  1.270366    0.635109\n",
       "yago     1.723333  1.000000    0.580271\n",
       "missi    1.000000  1.000000    1.000000\n",
       "lifeand  1.255583  1.000000    0.796443"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the ratio of spam-to-ham for each token\n",
    "tokens['spam_ratio'] = tokens.spam / tokens.ham\n",
    "tokens.sample(5, random_state=6)\n",
    "# You should not look at spam ratio and directly interpret\n",
    "\n",
    "# textoperator is the most spammy word\n",
    "# very is the least spammy word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.664883</td>\n",
       "      <td>15.664883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.273154</td>\n",
       "      <td>15.273154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.959931</td>\n",
       "      <td>11.959931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guarante</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.259130</td>\n",
       "      <td>9.259130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.410645</td>\n",
       "      <td>8.410645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urgent</th>\n",
       "      <td>1.348429</td>\n",
       "      <td>11.174057</td>\n",
       "      <td>8.286721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award</th>\n",
       "      <td>1.253052</td>\n",
       "      <td>10.319130</td>\n",
       "      <td>8.235197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.677953</td>\n",
       "      <td>7.677953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt</th>\n",
       "      <td>2.395995</td>\n",
       "      <td>18.046223</td>\n",
       "      <td>7.531828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rington</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.181245</td>\n",
       "      <td>7.181245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.565236</td>\n",
       "      <td>6.565236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>servic</th>\n",
       "      <td>1.623768</td>\n",
       "      <td>10.565379</td>\n",
       "      <td>6.506703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.230236</td>\n",
       "      <td>6.230236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.224462</td>\n",
       "      <td>7.500837</td>\n",
       "      <td>6.125824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nokia</th>\n",
       "      <td>1.639965</td>\n",
       "      <td>9.986780</td>\n",
       "      <td>6.089630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tcs</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.016866</td>\n",
       "      <td>6.016866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1.371603</td>\n",
       "      <td>7.709466</td>\n",
       "      <td>5.620769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150ppm</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.600898</td>\n",
       "      <td>5.600898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voucher</th>\n",
       "      <td>1.258423</td>\n",
       "      <td>6.717114</td>\n",
       "      <td>5.337723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mob</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.080026</td>\n",
       "      <td>5.080026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.077577</td>\n",
       "      <td>5.077577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobil</th>\n",
       "      <td>3.368350</td>\n",
       "      <td>16.794538</td>\n",
       "      <td>4.985983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>await</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.804178</td>\n",
       "      <td>4.804178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.799344</td>\n",
       "      <td>4.799344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.741002</td>\n",
       "      <td>4.741002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poli</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.722763</td>\n",
       "      <td>4.722763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.381765</td>\n",
       "      <td>6.459464</td>\n",
       "      <td>4.674791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.667145</td>\n",
       "      <td>4.667145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deliveri</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.656863</td>\n",
       "      <td>4.656863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.642467</td>\n",
       "      <td>4.642467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someth</th>\n",
       "      <td>16.496739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep</th>\n",
       "      <td>17.281001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>17.348338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>17.383906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finish</th>\n",
       "      <td>17.397393</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morn</th>\n",
       "      <td>17.470124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ive</th>\n",
       "      <td>17.649929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch</th>\n",
       "      <td>18.098977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>18.255239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>21.173965</td>\n",
       "      <td>1.153077</td>\n",
       "      <td>0.054457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happi</th>\n",
       "      <td>19.077321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyth</th>\n",
       "      <td>19.224148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>28.638834</td>\n",
       "      <td>1.474759</td>\n",
       "      <td>0.051495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much</th>\n",
       "      <td>23.951036</td>\n",
       "      <td>1.213828</td>\n",
       "      <td>0.050680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>21.093749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>21.663566</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wat</th>\n",
       "      <td>25.620719</td>\n",
       "      <td>1.182484</td>\n",
       "      <td>0.046153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>39.494244</td>\n",
       "      <td>1.756691</td>\n",
       "      <td>0.044480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>24.478756</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh</th>\n",
       "      <td>24.578691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>25.936753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorri</th>\n",
       "      <td>40.513294</td>\n",
       "      <td>1.472000</td>\n",
       "      <td>0.036334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>im</th>\n",
       "      <td>67.776297</td>\n",
       "      <td>2.440933</td>\n",
       "      <td>0.036015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>38.368025</td>\n",
       "      <td>1.304993</td>\n",
       "      <td>0.034013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>57.706136</td>\n",
       "      <td>1.661917</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>65.934689</td>\n",
       "      <td>1.878092</td>\n",
       "      <td>0.028484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>36.234749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>39.243147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ltgt</th>\n",
       "      <td>42.792697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ill</th>\n",
       "      <td>54.009190</td>\n",
       "      <td>1.149663</td>\n",
       "      <td>0.021286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8037 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ham       spam  spam_ratio\n",
       "token                                     \n",
       "claim      1.000000  15.664883   15.664883\n",
       "prize      1.000000  15.273154   15.273154\n",
       "tone       1.000000  11.959931   11.959931\n",
       "guarante   1.000000   9.259130    9.259130\n",
       "1000       1.000000   8.410645    8.410645\n",
       "urgent     1.348429  11.174057    8.286721\n",
       "award      1.253052  10.319130    8.235197\n",
       "500        1.000000   7.677953    7.677953\n",
       "txt        2.395995  18.046223    7.531828\n",
       "rington    1.000000   7.181245    7.181245\n",
       "150        1.000000   6.565236    6.565236\n",
       "servic     1.623768  10.565379    6.506703\n",
       "18         1.000000   6.230236    6.230236\n",
       "16         1.224462   7.500837    6.125824\n",
       "nokia      1.639965   9.986780    6.089630\n",
       "tcs        1.000000   6.016866    6.016866\n",
       "2000       1.371603   7.709466    5.620769\n",
       "150ppm     1.000000   5.600898    5.600898\n",
       "voucher    1.258423   6.717114    5.337723\n",
       "mob        1.000000   5.080026    5.080026\n",
       "5000       1.000000   5.077577    5.077577\n",
       "mobil      3.368350  16.794538    4.985983\n",
       "await      1.000000   4.804178    4.804178\n",
       "150p       1.000000   4.799344    4.799344\n",
       "valid      1.000000   4.741002    4.741002\n",
       "poli       1.000000   4.722763    4.722763\n",
       "100        1.381765   6.459464    4.674791\n",
       "video      1.000000   4.667145    4.667145\n",
       "deliveri   1.000000   4.656863    4.656863\n",
       "800        1.000000   4.642467    4.642467\n",
       "...             ...        ...         ...\n",
       "someth    16.496739   1.000000    0.060618\n",
       "sleep     17.281001   1.000000    0.057867\n",
       "lol       17.348338   1.000000    0.057642\n",
       "said      17.383906   1.000000    0.057524\n",
       "finish    17.397393   1.000000    0.057480\n",
       "morn      17.470124   1.000000    0.057241\n",
       "ive       17.649929   1.000000    0.056657\n",
       "watch     18.098977   1.000000    0.055252\n",
       "sure      18.255239   1.000000    0.054779\n",
       "yeah      21.173965   1.153077    0.054457\n",
       "happi     19.077321   1.000000    0.052418\n",
       "anyth     19.224148   1.000000    0.052018\n",
       "work      28.638834   1.474759    0.051495\n",
       "much      23.951036   1.213828    0.050680\n",
       "that      21.093749   1.000000    0.047407\n",
       "way       21.663566   1.000000    0.046160\n",
       "wat       25.620719   1.182484    0.046153\n",
       "got       39.494244   1.756691    0.044480\n",
       "say       24.478756   1.000000    0.040852\n",
       "oh        24.578691   1.000000    0.040686\n",
       "da        25.936753   1.000000    0.038555\n",
       "sorri     40.513294   1.472000    0.036334\n",
       "im        67.776297   2.440933    0.036015\n",
       "home      38.368025   1.304993    0.034013\n",
       "come      57.706136   1.661917    0.028800\n",
       "ok        65.934689   1.878092    0.028484\n",
       "lor       36.234749   1.000000    0.027598\n",
       "later     39.243147   1.000000    0.025482\n",
       "ltgt      42.792697   1.000000    0.023368\n",
       "ill       54.009190   1.149663    0.021286\n",
       "\n",
       "[8037 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "# note: use sort() instead of sort_values() for pandas 0.16.2 and earlier\n",
    "tokens.sort_values('spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example false negative\n",
    "X_test[3132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Summary\n",
    "\n",
    "# For each token, it calculates the conditional probability of that token given each class\n",
    "# Does this for every token and both classes\n",
    "# To make a prediction\n",
    "# Calculates conditional probability of a class given the token in that message\n",
    "# Bottomline to how it thinks\n",
    "# Learns spamminess of each token\n",
    "# If have a lot of ham then class = ham\n",
    "# If have a lot of spam then class = spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC is useful as a single number summary of classifier performance\n",
    "Higher value = better classifier\n",
    "If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a higher predicted probability to the positive observation\n",
    "AUC is useful even when there is high class imbalance (unlike classification accuracy)\n",
    "Fraud case\n",
    "Null accuracy almost 99%\n",
    "AUC is useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bhavik: Try determing which examples are being incorrectly predicted and why, build some visualization around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros:\n",
    "\n",
    "# Computationally fast\n",
    "# Simple to implement\n",
    "# Works well with small datasets\n",
    "# Works well with high dimensions\n",
    "# Perform well even if the Naive Assumption is not perfectly met. In many cases, the approximation is enough to build a good classifier.\n",
    "# Cons:\n",
    "\n",
    "# Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance.\n",
    "# If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation. Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lr is a lot slower than Naive Bayes but is good for probabilities\n",
    "\n",
    "# Naive Bayes cannot take negative numbers while Logistic Regression can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
