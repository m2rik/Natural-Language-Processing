{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.book as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Love finding hidden patterns and learning about Data and its value in the real world. I have a Highly Motivated\n",
    "and Entrepreneurial Mindset. I mainly use Python and the PyData stack for anything that I do but I also know\n",
    "a bit about Apache Spark, TensorFlow, R, SQL and much other stuff.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# tokenize doc\n",
    "tokenized_doc = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Rit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Rit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = nltk.pos_tag(tokenized_doc)\n",
    "ne_chunked_sents = nltk.ne_chunk(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'PERSON'), ('Highly Motivated', 'ORGANIZATION'), ('Entrepreneurial Mindset', 'ORGANIZATION'), ('Python', 'PERSON'), ('PyData', 'ORGANIZATION'), ('Apache Spark', 'PERSON'), ('TensorFlow', 'ORGANIZATION'), ('R', 'GPE'), ('SQL', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "# extract all named entities\n",
    "named_entities = []\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "        entity_type = tagged_tree.label() # get NE category\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux OS\n",
      "is IR\n",
      "the IR\n",
      "best IR\n",
      "OS IR\n",
      "Ubuntu OS\n",
      "is IR\n",
      "my IR\n",
      "favourite IR\n",
      "OS IR\n",
      "[[('Linux', 'OS'), ('is', 'IR'), ('the', 'IR'), ('best', 'IR'), ('OS', 'IR')], [('Ubuntu', 'OS'), ('is', 'IR'), ('my', 'IR'), ('favourite', 'IR'), ('OS', 'IR')]]\n"
     ]
    }
   ],
   "source": [
    "data = [(['Linux', 'is', 'the', 'best', 'OS'], ['OS','IR','IR','IR','IR']),\n",
    "(['Ubuntu', 'is', 'my', 'favourite', 'OS'], ['OS','IR','IR','IR','IR'])]\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for (doc, tags) in data:\n",
    "    doc_tag = []\n",
    "    for word, tag in zip(doc,tags):\n",
    "        print(word,tag)\n",
    "        doc_tag.append((word, tag))\n",
    "    corpus.append(doc_tag)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'word.word': 'Linux', 'BOS': True, 'word.nextword': 'is'}, {'word.word': 'is', 'word.prevword': 'Linux', 'word.nextword': 'the'}, {'word.word': 'the', 'word.prevword': 'is', 'word.nextword': 'best'}, {'word.word': 'best', 'word.prevword': 'the', 'word.nextword': 'OS'}, {'word.word': 'OS', 'word.prevword': 'best', 'EOS': True}], [{'word.word': 'Ubuntu', 'BOS': True, 'word.nextword': 'is'}, {'word.word': 'is', 'word.prevword': 'Ubuntu', 'word.nextword': 'my'}, {'word.word': 'my', 'word.prevword': 'is', 'word.nextword': 'favourite'}, {'word.word': 'favourite', 'word.prevword': 'my', 'word.nextword': 'OS'}, {'word.word': 'OS', 'word.prevword': 'favourite', 'EOS': True}]]\n"
     ]
    }
   ],
   "source": [
    "def doc2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    \n",
    "    # Features from current word\n",
    "    features={\n",
    "        'word.word': word,\n",
    "    }\n",
    "    # Features from previous word\n",
    "    if i > 0:\n",
    "        prevword = doc[i-1][0]\n",
    "        features['word.prevword'] = prevword\n",
    "    else:\n",
    "        features['BOS'] = True # Special \"Beginning of Sequence\" tag\n",
    "        \n",
    "    # Features from next word\n",
    "    if i < len(doc)-1:\n",
    "        nextword = doc[i+1][0]\n",
    "        features['word.nextword'] = nextword\n",
    "    else:\n",
    "        features['EOS'] = True # Special \"End of Sequence\" tag\n",
    "    return features\n",
    " \n",
    "def extract_features(doc):\n",
    "    return [doc2features(doc, i) for i in range(len(doc))]\n",
    " \n",
    "X = [extract_features(doc) for doc in corpus]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OS', 'IR', 'IR', 'IR', 'IR'], ['OS', 'IR', 'IR', 'IR', 'IR']]\n"
     ]
    }
   ],
   "source": [
    "def get_labels(doc):\n",
    "    return [tag for (token,tag) in doc]\n",
    "y = [get_labels(doc) for doc in corpus]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-crfsuite\n",
      "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
      "Collecting tabulate (from sklearn-crfsuite)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/fd/202954b3f0eb896c53b7b6f07390851b1fd2ca84aa95880d7ae4f434c4ac/tabulate-0.8.3.tar.gz (46kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in g:\\anaconda\\lib\\site-packages (from sklearn-crfsuite) (4.26.0)\n",
      "Requirement already satisfied: six in g:\\anaconda\\lib\\site-packages (from sklearn-crfsuite) (1.11.0)\n",
      "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/c9/b206fa75d5978a631b5e6914a051139d99ff4624f96eac1bec6486413944/python_crfsuite-0.9.6-cp36-cp36m-win_amd64.whl (154kB)\n",
      "Building wheels for collected packages: tabulate\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Rit\\AppData\\Local\\pip\\Cache\\wheels\\2b\\67\\89\\414471314a2d15de625d184d8be6d38a03ae1e983dbda91e84\n",
      "Successfully built tabulate\n",
      "Installing collected packages: tabulate, python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.6 sklearn-crfsuite-0.3.6 tabulate-0.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OS']\n"
     ]
    }
   ],
   "source": [
    "test = [['CentOS', 'is', 'my', 'favourite', 'OS']]\n",
    "X_test = extract_features(test)\n",
    "print(crf.predict_single(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
